{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f2dfd5e-a182-4aac-9653-ae2c022f2765",
   "metadata": {},
   "source": [
    "# Construindo um Data Lake: Entenda o Conceito e Veja na Prática"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47eb8ccb-8cd8-4435-85dc-f302b3e5339e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"11.0.2\" 2019-01-15\n",
      "OpenJDK Runtime Environment 18.9 (build 11.0.2+9)\n",
      "OpenJDK 64-Bit Server VM 18.9 (build 11.0.2+9, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "! java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7af90b-a182-4622-9304-39b38062066d",
   "metadata": {},
   "source": [
    "### Configurando as credenciais do projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a55efd00-ed44-4690-a0a5-c0c26aa00b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value1 \n",
      " value2999\n"
     ]
    }
   ],
   "source": [
    "from toml import load\n",
    "\n",
    "with open(\"credentials.toml\", \"r\") as toml_file:\n",
    "    credentials = load(toml_file)\n",
    "\n",
    "user = credentials.get(\"credentials1\").get(\"key1\")\n",
    "password = credentials.get(\"credentials1\").get(\"key2\")\n",
    "print(user, \"\\n\", password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4ba659-fa4a-427b-a1dc-9cdce13bca02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/11/05 11:05:44 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:834)\n",
      "23/11/05 11:05:44 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master 10.0.0.3:7077\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:107)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: java.io.IOException: Failed to connect to /10.0.0.3:7077\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:288)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:230)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:204)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:202)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:198)\n",
      "\t... 4 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /10.0.0.3:7077\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:710)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:658)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "23/11/05 11:06:04 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master 10.0.0.3:7077\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:107)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: java.io.IOException: Failed to connect to /10.0.0.3:7077\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:288)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:230)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:204)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:202)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:198)\n",
      "\t... 4 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /10.0.0.3:7077\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:710)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:658)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Processamento de Dados - Data Lake\") \\\n",
    "    .config(\"spark.master\", \"spark://127.0.0.1:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "    #.config(\"spark.hadoop.fs.s3a.access.key\", \"sua_access_key\") \\\n",
    "    #.config(\"spark.hadoop.fs.s3a.secret.key\", \"seu_secret_key\") \\\n",
    "    #.config(\"spark.hadoop.fs.s3a.endpoint\", \"URL_do_MinIO\") \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12a7177d-2001-4432-8e70-de9dfe030d9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19971e7f-afd9-44f0-9bfc-883c096371ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark==3.5.0 (from -r ../requirements.txt (line 1))\n",
      "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: toml==0.10.2 in /home/jupyter/python/jupyter/.venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 2)) (0.10.2)\n",
      "Collecting py4j==0.10.9.7 (from pyspark==3.5.0->-r ../requirements.txt (line 1))\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=403ed479f046f1f2ef7dfbed7f5b733c2c06951c835cf85e4d5d05bb3590a272\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/a6/ce/f9/17d82c92f044018df2fe30af63ac043447720d5b2cee39b40f\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 3.3.0\n",
      "    Not uninstalling pyspark at /home/spark/spark-3.3.0/python, outside environment /home/jupyter/python/jupyter/.venv\n",
      "    Can't uninstall 'pyspark'. No files were found to uninstall.\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.0\n"
     ]
    }
   ],
   "source": [
    "! pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac4df89-6a81-4500-9584-a3b2304ac1bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
