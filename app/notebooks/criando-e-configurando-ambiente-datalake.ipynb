{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f2dfd5e-a182-4aac-9653-ae2c022f2765",
   "metadata": {},
   "source": [
    "# Construindo um Data Lake: Entenda o Conceito e Veja na Prática"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514fff71-8563-457b-b7a7-79979ff00237",
   "metadata": {},
   "source": [
    "### Instalando as dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19971e7f-afd9-44f0-9bfc-883c096371ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.5.0 in /home/jupyter/python/jupyter/.venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: toml==0.10.2 in /home/jupyter/python/jupyter/.venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 2)) (0.10.2)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/jupyter/python/jupyter/.venv/lib/python3.8/site-packages (from pyspark==3.5.0->-r ../requirements.txt (line 1)) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "! pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "174419e7-a6a5-442c-acc1-75334d2a8be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"11.0.2\" 2019-01-15\n",
      "OpenJDK Runtime Environment 18.9 (build 11.0.2+9)\n",
      "OpenJDK 64-Bit Server VM 18.9 (build 11.0.2+9, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "! java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7af90b-a182-4622-9304-39b38062066d",
   "metadata": {},
   "source": [
    "### Configurando as credenciais do projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a55efd00-ed44-4690-a0a5-c0c26aa00b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value1 \n",
      " value2999\n"
     ]
    }
   ],
   "source": [
    "from toml import load\n",
    "\n",
    "with open(\"credentials.toml\", \"r\") as toml_file:\n",
    "    credentials = load(toml_file)\n",
    "\n",
    "user = credentials.get(\"credentials1\").get(\"key1\")\n",
    "password = credentials.get(\"credentials1\").get(\"key2\")\n",
    "print(user, \"\\n\", password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea4ba659-fa4a-427b-a1dc-9cdce13bca02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/05 22:22:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/11/05 22:22:57 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master 10.0.0.2:7077\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:108)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: java.io.IOException: Failed to connect to /10.0.0.2:7077\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:294)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:226)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:204)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:202)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:198)\n",
      "\t... 4 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /10.0.0.2:7077\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "23/11/05 22:23:17 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\n",
      "23/11/05 22:23:17 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\n",
      "23/11/05 22:23:20 WARN StandaloneAppClient$ClientEndpoint: Drop UnregisterApplication(null) because has not yet connected to master\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"] = \"notebook\"\n",
    "#os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "#os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Processamento de Dados - Data Lake\") \\\n",
    "    .config(\"spark.master\", \"spark://10.0.0.2:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "    #.config(\"spark.hadoop.fs.s3a.access.key\", \"sua_access_key\") \\\n",
    "    #.config(\"spark.hadoop.fs.s3a.secret.key\", \"seu_secret_key\") \\\n",
    "    #.config(\"spark.hadoop.fs.s3a.endpoint\", \"URL_do_MinIO\") \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1f54e6-f7a6-4f5c-b43d-a3084e6b0864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac4df89-6a81-4500-9584-a3b2304ac1bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
